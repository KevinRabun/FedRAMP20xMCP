name: MCP Server Evaluation

on:
  schedule:
    # Run weekly on Sundays at 2:00 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      evaluation_type:
        description: 'Evaluation type'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - critical-only
          - accuracy
          - completeness
          - analysis_quality
          - relevance
          - consistency

permissions:
  contents: read
  issues: write

env:
  PYTHON_VERSION: '3.11'

jobs:
  evaluate:
    name: Run MCP Server Evaluation
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Determine evaluation arguments
      id: args
      run: |
        EVAL_TYPE="${{ github.event.inputs.evaluation_type || 'full' }}"
        if [ "$EVAL_TYPE" == "full" ]; then
          echo "eval_args=" >> $GITHUB_OUTPUT
        elif [ "$EVAL_TYPE" == "critical-only" ]; then
          echo "eval_args=--critical-only" >> $GITHUB_OUTPUT
        else
          echo "eval_args=--category $EVAL_TYPE" >> $GITHUB_OUTPUT
        fi
    
    - name: Run evaluation
      id: evaluate
      run: |
        python tests/run_evaluation.py ${{ steps.args.outputs.eval_args }} --format json
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      continue-on-error: true
    
    - name: Upload evaluation report
      uses: actions/upload-artifact@v4
      with:
        name: evaluation-report
        path: tests/evaluator/reports/
        retention-days: 30
    
    - name: Parse evaluation results
      id: results
      run: |
        # Find the most recent report
        REPORT=$(ls -t tests/evaluator/reports/*.json | head -1)
        if [ -f "$REPORT" ]; then
          SCORE=$(python -c "import json; d=json.load(open('$REPORT')); print(d['summary']['overall_score'])")
          PASS_RATE=$(python -c "import json; d=json.load(open('$REPORT')); print(d['summary']['overall_pass_rate'])")
          TOTAL=$(python -c "import json; d=json.load(open('$REPORT')); print(d['summary']['total_tests'])")
          FAILURES=$(python -c "import json; d=json.load(open('$REPORT')); print(len(d.get('failures', [])))")
          
          echo "score=$SCORE" >> $GITHUB_OUTPUT
          echo "pass_rate=$PASS_RATE" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL" >> $GITHUB_OUTPUT
          echo "failures=$FAILURES" >> $GITHUB_OUTPUT
          
          # Calculate percentage
          SCORE_PCT=$(python -c "print(f'{$SCORE * 100:.1f}')")
          echo "score_pct=$SCORE_PCT" >> $GITHUB_OUTPUT
        fi
    
    - name: Create issue on significant degradation
      if: steps.results.outputs.score != '' && steps.results.outputs.score < '0.8'
      uses: actions/github-script@v7
      with:
        script: |
          const score = '${{ steps.results.outputs.score_pct }}';
          const passRate = '${{ steps.results.outputs.pass_rate }}';
          const totalTests = '${{ steps.results.outputs.total_tests }}';
          const failures = '${{ steps.results.outputs.failures }}';
          
          const title = `ðŸ” MCP Server Evaluation Score: ${score}%`;
          const body = `## Weekly MCP Server Evaluation Results
          
          The evaluation score has dropped below 80%, indicating potential quality issues.
          
          | Metric | Value |
          |--------|-------|
          | Overall Score | ${score}% |
          | Pass Rate | ${(passRate * 100).toFixed(1)}% |
          | Total Tests | ${totalTests} |
          | Failures | ${failures} |
          
          **Workflow Run:** [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})
          
          ### Action Required
          1. Download the evaluation report artifact for detailed results
          2. Review failing test cases
          3. Investigate any accuracy or completeness issues
          4. Update analyzers or data loading if needed
          
          /cc @${context.actor}`;
          
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: title,
            body: body,
            labels: ['evaluation', 'quality', 'automated']
          });
    
    - name: Summary
      run: |
        echo "## Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
        echo "| Overall Score | ${{ steps.results.outputs.score_pct }}% |" >> $GITHUB_STEP_SUMMARY
        echo "| Total Tests | ${{ steps.results.outputs.total_tests }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Failures | ${{ steps.results.outputs.failures }} |" >> $GITHUB_STEP_SUMMARY
    
    - name: Fail if score below threshold
      if: steps.results.outputs.score != '' && steps.results.outputs.score < '0.7'
      run: |
        echo "âŒ Evaluation score (${{ steps.results.outputs.score_pct }}%) is below 70% threshold"
        exit 1
